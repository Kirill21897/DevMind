# 01. Концепция и Архитектура проекта "DevMind"

## 1. Введение
**DevMind** — это автономный AI-агент, предназначенный для помощи разработчикам в поиске технических решений, работе с документацией и генерации примеров кода.

Ключевая особенность проекта — простота реализации (без использования сложных фреймворков вроде MCP) и строгая ориентация на последующую оценку качества генерации (**Evaluation-First approach**) с использованием библиотеки **Ragas**.

## 2. Основные цели
1. **Помощь в разработке**: Быстрый поиск ответов по локальной базе знаний (документация проекта, библиотеки) и интернету.
2. **Локальная приватность**: Использование **Ollama** для запуска локальных LLM и Embeddings, обеспечивая приватность кода.
3. **Готовность к метрикам (Ragas Ready)**: Архитектура должна автоматически сохранять трейсы генерации (вопрос, ответ, использованный контекст) для последующего расчета метрик:
   - *Faithfulness* (Верность контексту)
   - *Answer Relevance* (Релевантность ответа)
   - *Context Precision* (Точность контекста)

## 3. Архитектура Агента

Агент построен на модульной архитектуре с центральным **Orchestrator** (LLM), который управляет потоком выполнения и выбором инструментов.

### Поток данных (Data Flow) и Сбор данных для Ragas:
1. **User Query**: Пользователь задает технический вопрос.
2. **Tool Selection**: LLM (через Ollama) решает, какой инструмент использовать (Поиск или Веб).
3. **Execution & Context Capture**:
   - При вызове инструментов поиска, система выполняет **Two-Stage Retrieval**:
     1.  Векторный поиск (Top-K).
     2.  **Reranking** (Переранжирование результатов Cross-Encoder'ом) для повышения точности.
   - Финальные чанки сохраняются во временный буфер `retrieved_contexts`.
4. **Generation**: LLM генерирует ответ на основе контекстов.
5. **Observability Layer**:
   - По завершении сессии, система формирует запись:
     ```json
     {
       "question": "user query",
       "answer": "agent response",
       "contexts": ["chunk 1", "chunk 2"],
       "ground_truth": null  // Заполняется экспертом позже
     }
     ```
   - Данные сохраняются в `evaluation_dataset.json`.

## 4. Инструментарий (The 3 Tools)
Агент ограничен ровно тремя инструментами для сохранения фокуса и простоты отладки:

### 1. `KnowledgeRetriever` (Поиск знаний + Rerank)
- **Вход**: Строковый поисковый запрос (query).
- **Действие**: 
  1. Выполняет семантический поиск (Vector Search) по ChromaDB.
  2. Применяет **Cross-Encoder Reranker** для пересортировки найденных чанков по релевантности запросу.
- **Выход**: Список отфильтрованных и отсортированных текстовых фрагментов.
- **Значение для Ragas**: Главный источник для метрики *Context Precision* (Reranker критически важен для улучшения этой метрики).

### 2. `WebExplorer` (Веб-исследование)
- **Вход**: Поисковый запрос (query).
- **Действие**: Выполняет поиск в Google/DuckDuckGo, парсит топ-3 результата.
- **Выход**: Суммаризированный текст найденных статей.
- **Значение для Ragas**: Используется как fallback-контекст.

### 3. `SolutionReport` (Генератор отчета)
- **Вход**: Имя файла, контент в Markdown.
- **Действие**: Сохраняет сгенерированное решение, пример кода или инструкцию в файл.
- **Выход**: Статус операции.
- **Значение для Ragas**: Позволяет оценивать качество *конечного артефакта*.

## 5. Этапы Реализации
1. **Подготовка окружения**: Настройка Ollama, ChromaDB.
2. **Реализация инструментов**: Внедрение Reranker в поиск.
3. **Сборка RAG-пайплайна**: Логика связывания инструментов и LLM.
4. **Внедрение слоя Evaluation**: Логирование контекстов для Ragas.
5. **Тестирование**: Запуск на тестовых вопросах и генерация отчета метрик.
